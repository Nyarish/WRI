{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDELT Pilot\n",
    "### John Brandt\n",
    "\n",
    "## Data Acquisition with BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "client = bigquery.Client()\n",
    "\n",
    "query = (\n",
    "    \n",
    "    \"SELECT SourceCommonName, Amounts, V2Locations, V2Organizations, V2Themes FROM [gdelt-bq:gdeltv2.gkg@-604800000-] \"\n",
    "    'WHERE Amounts LIKE \"%trees%\"'\n",
    "    'AND Amounts LIKE \"%planted%\"'\n",
    ")\n",
    "query_job = client.query(\n",
    "    query,\n",
    "    location=\"US\",\n",
    ")\n",
    "\n",
    "for row in query_job:  # API request - fetches results\n",
    "    # Row values can be accessed by field name or index\n",
    "    assert row[0] == row.name == row[\"name\"]\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data\n",
    "\n",
    "Because each BigQuery call costs about \\$0.25 USD, I will load in a CSV during each jupyter session. This notebook currently works with weekly references to tree plantings, but will be functionalized to work with other event detections in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"../data/external\")\n",
    "data = pd.read_csv(\"../data/external/\" + files[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locating events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to tie events to locations and organizations, we assign the location most closely referenced to the event in the text. \n",
    "\n",
    "For each event detected, create a dictionary of the form {index : (number, action)}. This will be matched with a similar location dictionary of the form {index : location} with a grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{507: (16, 'trees planted for more')}\n",
      "{657: (1000000000, 'trees'), 1376: (300, 'trees between ash'), 2906: (300, 'trees planted for every')}\n",
      "{2181: (21, 'willow trees implanted upside')}\n",
      "{1068: (10000000, 'trees till date'), 1114: (22000000, 'trees planted by the')}\n",
      "{729: (14000000, 'trees would be planted')}\n",
      "{77: (60, 'volunteers planted 26 trees'), 96: (26, 'trees')}\n",
      "{54: (2, 'mature trees'), 1889: (80, 'trees planted')}\n",
      "{3: (250, 'native ghaf trees planted'), 87: (250, 'native ghaf trees planted'), 175: (50, 'ghaf trees were each')}\n",
      "{69: (43, 'trees'), 437: (43, 'trees have been replanted')}\n",
      "{69: (43, 'trees'), 437: (43, 'trees have been replanted')}\n"
     ]
    }
   ],
   "source": [
    "for i, val in enumerate(data.iloc[0:10, 2]):               # Loop through each entry\n",
    "    refs = ([x for x in val.split(\";\") if \"trees\" in x])   # Split up the references into value, action, index\n",
    "    values, actions, indexes = [], [], []\n",
    "    for ref in refs:                                       # Parse into separate lists\n",
    "        parsed = ref.split(\",\")\n",
    "        values.append(int(parsed[0]))\n",
    "        actions.append(parsed[1])\n",
    "        indexes.append(int(parsed[2]))\n",
    "    refs = dict(zip(indexes, zip(values, actions)))        # {index: (number, action)}\n",
    "    print(refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1#Spanish#SP#SP##40#-4#SP#2488', '1#Spanish#SP#SP##40#-4#SP#4450', '1#India#IN#IN##20#77#IN#291', '1#India#IN#IN##20#77#IN#3916', '1#India#IN#IN##20#77#IN#4377', '4#Madrid, Madrid, Spain#SP#SP29#25820#40.4#-3.68333#-390625#677', '4#Madrid, Madrid, Spain#SP#SP29#25820#40.4#-3.68333#-390625#1613', '4#Mahadev, Jammu And Kashmir, India#IN#IN12#72851#32.8889#74.9014#6205823#3909', '4#Kathmandu, Bagmati, Nepal#NP#NP01#22353#27.7167#85.3167#-1022136#2796', '4#Kathmandu, Bagmati, Nepal#NP#NP01#22353#27.7167#85.3167#-1022136#3752', '1#Nepal#NP#NP##28#84#NP#299', '1#Nepal#NP#NP##28#84#NP#2289', '1#Nepal#NP#NP##28#84#NP#2714', '1#Nepal#NP#NP##28#84#NP#2803', '1#Nepal#NP#NP##28#84#NP#2983', '1#Nepal#NP#NP##28#84#NP#3759', '1#Nepal#NP#NP##28#84#NP#3850', '1#Nepal#NP#NP##28#84#NP#3933', '1#Nepal#NP#NP##28#84#NP#4385']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a matching dictionary\n",
    "for i in data.iloc[1:2, 3]:\n",
    "    print(str(i).split(\";\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
